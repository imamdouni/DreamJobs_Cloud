{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-24T09:37:38.760742400Z",
     "start_time": "2023-08-24T09:36:37.220351100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fertig\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import tools\n",
    "from selenium import webdriver\n",
    "from scraper import Scraper\n",
    "class BaseScraper:\n",
    "    def __init__(self):\n",
    "        self.driver = webdriver.Chrome()  # Hier kannst du den Webdriver deiner Wahl verwenden\n",
    "    def close_driver(self):\n",
    "        self.driver.quit()\n",
    "class Scraper(BaseScraper):\n",
    "    def __init__(self, jobtitel, suchort=\"Deutschland\", anzahl_seiten=10):\n",
    "        super().__init__()\n",
    "        self.jobtitel = jobtitel\n",
    "        self.suchort = suchort\n",
    "        self.anzahl_seiten = anzahl_seiten\n",
    "class Indeed_Scraper(Scraper):\n",
    "    link_liste_scraped = []\n",
    "    def __init__(self, jobtitel, suchort=\"Deutschland\", anzahl_seiten=10):\n",
    "        super().__init__(\n",
    "            jobtitel=jobtitel,\n",
    "            suchort=suchort,\n",
    "            anzahl_seiten=anzahl_seiten\n",
    "        )\n",
    "        self.scraper_name = \"indeed\"\n",
    "        self.url = \"https://www.indeed.de/\"\n",
    "    def suche_sort(self):\n",
    "        # Webseite aufrufen\n",
    "        self.driver.get(self.url)\n",
    "        # Fenster Maximieren\n",
    "        self.driver.maximize_window()  # evtl. im Headless-Modus nicht machbar\n",
    "        # Cookies akzeptieren\n",
    "        tools.wartezeit(0.5, 1)\n",
    "        try:\n",
    "            self.driver.find_element(\n",
    "                By.XPATH, '//*[@id=\"onetrust-accept-btn-handler\"]'\n",
    "            ).click()\n",
    "        except:\n",
    "            pass\n",
    "        tools.wartezeit(0.5, 1)\n",
    "        # Suchfeld Jobbezeichnung -> finden und befüllen\n",
    "        try:\n",
    "            suchfeld_jobtitel = self.driver.find_element(\n",
    "                By.XPATH, '//*[@id=\"text-input-what\"]'\n",
    "            )\n",
    "            suchfeld_jobtitel.send_keys(self.jobtitel)\n",
    "            tools.wartezeit(0.5, 1)\n",
    "            # Suchfeld Ort -> finden und befüllen\n",
    "            suchfeld_ort = self.driver.find_element(\n",
    "                By.XPATH, '//*[@id=\"text-input-where\"]'\n",
    "            )\n",
    "            ### hier kommt leere such ort finde den fehler ab hier überlegen\n",
    "             # Vorhandenen Text im Suchfeld markieren und löschen\n",
    "            suchfeld_ort.send_keys(Keys.CONTROL + \"a\")  # Markiere den vorhandenen Text im Suchfeld\n",
    "            suchfeld_ort.send_keys(Keys.DELETE)  # Lösche den vorhandenen Text im Suchfeld\n",
    "            suchfeld_ort.send_keys(self.suchort)##das war schon\n",
    "            # Anfrage mit Enter/Return abschicken\n",
    "            suchfeld_ort.send_keys(Keys.RETURN)\n",
    "        except:\n",
    "            tools.schreibe_log_file(\n",
    "                self.scraper_name,\n",
    "                f\"Suchdaten eingeben nicht möglich: Jobtitel:-{self.jobtitel}; Suchort-{self.suchort}\",\n",
    "            )\n",
    "            return\n",
    "        tools.wartezeit(0.5, 1)\n",
    "        # nach Datum sortieren\n",
    "        self.driver.find_element(\n",
    "            By.XPATH,\n",
    "            '//*[@id=\"jobsearch-JapanPage\"]/div/div/div[5]/div[1]/div[4]/div/div/div[1]/span[2]/a',\n",
    "        ).click()\n",
    "        tools.wartezeit(0.5, 1)\n",
    "        # Einblendung entfernen\n",
    "        try:\n",
    "            self.driver.find_element(\n",
    "                By.XPATH, '//*[@id=\"mosaic-desktopserpjapopup\"]/div[1]/button'\n",
    "            ).click()\n",
    "        except:\n",
    "            pass\n",
    "        tools.wartezeit(0.5, 1)\n",
    "    def scrape_urls(self):\n",
    "        # Eingabemaske für Suche ausfüllen und nach Datum absteigend sortieren\n",
    "        self.suche_sort()\n",
    "        # Leere Liste erstellen für die Links\n",
    "        link_liste_scraped = []\n",
    "        ## link_liste_scraped befüllen von erster Seite\n",
    "        anzeigen = self.driver.find_elements(By.CLASS_NAME, \"jcs-JobTitle\")\n",
    "        for anzeige in anzeigen:\n",
    "            link_liste_scraped.append(anzeige.get_attribute(\"href\"))\n",
    "        # auf \"nächste Seite\" Button klicken\n",
    "        try:\n",
    "            self.driver.find_element(\n",
    "                By.XPATH,\n",
    "                '//*[@id=\"jobsearch-JapanPage\"]/div/div/div[5]/div[1]/nav/div[6]/a',\n",
    "            ).click()\n",
    "        except:\n",
    "            pass\n",
    "        ## von weiter Seiten die Links holen\n",
    "        for i in range(self.anzahl_seiten - 1):\n",
    "            anzeigen = self.driver.find_elements(By.CLASS_NAME, \"jcs-JobTitle\")\n",
    "            for anzeige in anzeigen:\n",
    "                link_liste_scraped.append(anzeige.get_attribute(\"href\"))\n",
    "            # auf \"nächste Seite\" Button klicken\n",
    "            try:\n",
    "                self.driver.find_element(\n",
    "                    By.XPATH,\n",
    "                    '//*[@id=\"jobsearch-JapanPage\"]/div/div/div[5]/div[1]/nav/div[7]/a',\n",
    "                ).click()\n",
    "            except:\n",
    "                pass\n",
    "        tools.wartezeit(0.5, 1)\n",
    "    def scrape_details(self, url):\n",
    "        # Aufrufen einer Jobseite Seite\n",
    "        try:\n",
    "            self.driver.get(url)\n",
    "            tools.wartezeit(1, 3)\n",
    "        except:\n",
    "            tools.schreibe_log_file(\n",
    "                self.scraper_name, f\"Fehler beim Aufrufen der URL: {url}\"\n",
    "            )\n",
    "            return\n",
    "        # Scrapen der daten\n",
    "        try:\n",
    "            inhalt_html = self.driver.find_element(\n",
    "                By.CLASS_NAME, \"jobsearch-JobComponent\"\n",
    "            ).get_attribute(\"innerHTML\")\n",
    "            inhalt_text = self.driver.find_element(\n",
    "                By.CLASS_NAME, \"jobsearch-JobComponent\"\n",
    "            ).text\n",
    "            tools.schreibe_log_file(self.scraper_name, f\"Daten wurden gezogen: {url}\")\n",
    "            d = {\n",
    "                \"seite\": self.scraper_name,\n",
    "                \"seiten_inhalt_html\": inhalt_html,\n",
    "                \"seiten_inhalt\": inhalt_text,\n",
    "                \"url\": url,\n",
    "                \"datum\": datetime.now(),\n",
    "                \"storno\": False,\n",
    "            }\n",
    "            return pd.DataFrame(data=[d])\n",
    "        except:\n",
    "            tools.schreibe_log_file(self.scraper_name, \"Konnte Daten nicht extrahieren\")\n",
    "if __name__ == \"__main__\":\n",
    "    jobtitel = \"Data Analyst\"  # Hier den gewünschten Jobtitel eintragen\n",
    "    suchort = \"Deutschland\"  # Hier den gewünschten Suchort eintragen\n",
    "    anzahl_seiten = 10  # Hier die gewünschte Anzahl von Seiten eintragen\n",
    "    # Die folgende Zeile bleibt unverändert, wenn du das neue Skript ausführst\n",
    "    scraper_instance = Indeed_Scraper(jobtitel, suchort, anzahl_seiten)\n",
    "    scraper_instance.suche_sort()\n",
    "    scraper_instance.scrape_urls()\n",
    "    # Loop durch die Links und Details sammeln\n",
    "    for url in scraper_instance.link_liste_scraped:\n",
    "        df = scraper_instance.scrape_details(url)\n",
    "        if df is not None:\n",
    "            # Speichere die Daten in der Datenbank oder tue, was immer du mit den Daten tun möchtest\n",
    "            print(\"Daten erfolgreich extrahiert und gespeichert.\")\n",
    "    # Hier endet der Hauptteil deines Skripts\n",
    "    # Entferne die folgenden Zeilen, da sie im neuen Skript nicht verwendet werden\n",
    "    print(\"fertig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T08:56:05.258800500Z",
     "start_time": "2023-08-24T08:56:05.218806800Z"
    }
   },
   "id": "240f14dc7accf26b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3e56985ffee4df8b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
